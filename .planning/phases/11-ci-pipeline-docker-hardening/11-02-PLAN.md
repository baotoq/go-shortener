---
phase: 11-ci-pipeline-docker-hardening
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - services/url-api/etc/url-docker.yaml
  - services/analytics-consumer/etc/consumer-docker.yaml
autonomous: true

must_haves:
  truths:
    - "Kafka publishing from url-api works in Docker Compose environment"
    - "Click events reach analytics-consumer and are stored in PostgreSQL clicks table"
    - "Fire-and-forget publishing pattern is preserved (user never blocked)"
    - "Dual Kafka listener setup maintained (Docker internal on 9092, host on 29092)"
  artifacts:
    - path: "docker-compose.yml"
      provides: "Fixed Kafka advertised listener configuration"
      contains: "KAFKA_ADVERTISED_LISTENERS"
  key_links:
    - from: "services/url-api (Docker container)"
      to: "kafka:9092"
      via: "kq.Pusher producing to click-events topic"
      pattern: "kafka:9092"
    - from: "kafka click-events topic"
      to: "services/analytics-consumer (Docker container)"
      via: "kq consumer reading from topic"
      pattern: "kafka:9092"
---

<objective>
Fix Kafka message publishing from url-api in the Docker Compose environment so click events flow end-to-end.

Purpose: The known issue from Phase 10-02 is that url-api cannot publish messages to Kafka when running in Docker, even though network connectivity is verified. The root cause is likely the Kafka advertised listener configuration -- when kq.Pusher requests broker metadata, Kafka returns an advertised listener hostname that the url-api container cannot resolve.

Output: Working end-to-end click event pipeline in Docker: shorten URL -> redirect -> Kafka publish -> consumer enriches -> click stored in PostgreSQL.
</objective>

<execution_context>
@/Users/baotoq/.claude/get-shit-done/workflows/execute-plan.md
@/Users/baotoq/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-resilience-infrastructure/10-02-SUMMARY.md
@docker-compose.yml
@services/url-api/etc/url-docker.yaml
@services/analytics-consumer/etc/consumer-docker.yaml
@services/url-api/internal/logic/redirect/redirectlogic.go
@services/analytics-consumer/internal/mqs/clickeventconsumer.go
</context>

<tasks>

<task type="auto">
  <name>Task 1: Investigate and fix Kafka Docker networking for kq.Pusher</name>
  <files>
    docker-compose.yml
    services/url-api/etc/url-docker.yaml
    services/analytics-consumer/etc/consumer-docker.yaml
  </files>
  <action>
    **Investigation approach:**

    The known issue from 10-02 summary: "Network connectivity verified (url-api can ping/nc kafka:9092), Kafka broker is healthy and accepting connections, analytics-consumer connected successfully to Kafka, No Kafka errors in url-api logs."

    The Kafka advertised listeners in docker-compose.yml are currently:
    ```
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
    ```

    And the port mapping is `"9092:29092"` (host 9092 -> container 29092).

    The url-api Docker config has `Brokers: [kafka:9092]`. This should connect to Kafka's PLAINTEXT listener on port 9092, and Kafka should return `kafka:9092` as the advertised listener for that protocol. This SHOULD work because Docker DNS resolves `kafka` to the Kafka container IP.

    **Potential root causes to investigate (in order):**

    1. **kq.Pusher initialization timing:** The Pusher may be created before Kafka is fully ready. Even though Docker health check passes, the broker might not be ready for metadata requests. Add `start_period` to Kafka health check if not present, and ensure url-api's `depends_on` uses `condition: service_healthy` for kafka (not just `service_started`).

    2. **Fire-and-forget goroutine error swallowing:** The `threading.GoSafe` pattern in redirect logic catches panics but the kq.Pusher may be silently failing. Check if there's any error logging needed.

    3. **Kafka listener binding issue:** The PLAINTEXT listener binds to `0.0.0.0:9092` and PLAINTEXT_HOST to `0.0.0.0:29092`. Internal Docker clients connect to `kafka:9092` -> PLAINTEXT listener. This should be correct.

    **Fix steps:**

    1. Run `docker compose up -d` and test the current behavior to reproduce the issue. Use `docker compose logs -f url-api` and `docker compose logs -f analytics-consumer` and `docker compose logs -f kafka` to observe.

    2. Use `docker compose exec url-api sh` (or equivalent) to test Kafka connectivity from inside the container. Use `docker compose exec kafka /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list` to verify topics exist.

    3. Create a short URL via `curl -X POST http://localhost:8080/api/v1/urls -H "Content-Type: application/json" -d '{"url":"https://example.com"}'`, then visit the redirect URL to trigger a click event.

    4. Check if click events appear in the topic: `docker compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic click-events --from-beginning --timeout-ms 5000`

    5. Check the clicks table: `docker compose exec postgres psql -U postgres -d shortener -c "SELECT * FROM clicks;"`

    6. Based on findings, apply the fix. The most likely fix is one of:

       a. **Ensure url-api depends on kafka healthy (already done in compose)** -- verify `condition: service_healthy`

       b. **kq.Pusher needs a synchronous test write or the error is being swallowed** -- check if kq.Pusher.Push returns an error and whether redirect logic checks it. The fire-and-forget pattern uses `threading.GoSafe` which recovers panics but the push error inside the goroutine may need `logx.Error` logging.

       c. **Kafka port mapping mismatch** -- Current: host 9092 -> container 29092. The local YAML uses `localhost:9092` which maps to container PLAINTEXT_HOST (29092). Docker YAML uses `kafka:9092` which maps to container PLAINTEXT (9092). This should be correct. But verify that KAFKA_LISTENERS ports match what clients expect.

    7. After identifying and applying the fix, rebuild and test: `docker compose down && docker compose up -d --build`

    8. The fire-and-forget pattern MUST be preserved (per user decision). Do not change to synchronous publishing. If the error is being silently swallowed, add `logx.Error` logging inside the goroutine but do NOT block the redirect response.

    **Important constraints:**
    - Keep dual Kafka listener setup (9092 internal Docker, 29092 for host)
    - No Kafka connectivity check at startup (keep lazy connection)
    - Fire-and-forget pattern preserved (user never blocked by analytics)
    - Log errors in background goroutine but never block
  </action>
  <verify>
    1. `docker compose up -d --build` -- all services start and become healthy
    2. Create a short URL: `curl -s -X POST http://localhost:8080/api/v1/urls -H "Content-Type: application/json" -d '{"url":"https://example.com/test-kafka"}'`
    3. Visit the redirect URL to trigger a click event
    4. Check click events in Kafka topic: `docker compose exec kafka /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic click-events --from-beginning --timeout-ms 10000`
    5. Check clicks table has enriched data: `docker compose exec postgres psql -U postgres -d shortener -c "SELECT * FROM clicks;"`
    6. Clean up: `docker compose down`
  </verify>
  <done>
    Click events successfully flow from url-api -> Kafka -> analytics-consumer -> PostgreSQL clicks table in Docker Compose environment. The fire-and-forget pattern is preserved. End-to-end verification passes.
  </done>
</task>

</tasks>

<verification>
1. `docker compose up -d --build` starts all 5 services (postgres, kafka, analytics-rpc, url-api, analytics-consumer)
2. Creating a short URL and visiting the redirect produces a click event in Kafka
3. analytics-consumer processes the click event and inserts an enriched row into clicks table
4. No blocking behavior in redirect handler (response time unaffected by Kafka)
5. Both Docker and local dev workflows continue to work (dual listener preserved)
</verification>

<success_criteria>
- End-to-end click event pipeline works in Docker Compose: redirect -> Kafka -> consumer -> PostgreSQL
- Fire-and-forget pattern preserved (redirect response not blocked by Kafka)
- Dual Kafka listener setup maintained (internal Docker + localhost)
- docker compose logs show click events being published and consumed
</success_criteria>

<output>
After completion, create `.planning/phases/11-ci-pipeline-docker-hardening/11-02-SUMMARY.md`
</output>
