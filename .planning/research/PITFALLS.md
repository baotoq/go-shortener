# Pitfalls Research

**Domain:** Go-Zero microservices adoption (migration from Chi/Dapr/SQLite)
**Researched:** 2026-02-15
**Confidence:** MEDIUM

## Critical Pitfalls

### Pitfall 1: Code Generation Overwriting Custom Logic

**What goes wrong:**
Developers write custom business logic in generated handler files or main.go, then re-run `goctl api go` and lose all their work. The framework doesn't protect arbitrary files from regeneration, only specific patterns.

**Why it happens:**
Go-zero's "convention over configuration" philosophy creates clear boundaries (handler vs. logic), but developers unfamiliar with the framework put code in the wrong places. Generated files include "DO NOT EDIT" headers, but developers ignore them or don't see them until it's too late.

**How to avoid:**
- NEVER edit files with "Code generated by goctl. DO NOT EDIT." headers
- Put ALL custom business logic in `logic/*.go` files (marked as safe-to-edit)
- Put ALL custom dependencies in `internal/svc/servicecontext.go` (safe-to-edit)
- Use template customization (`~/.goctl/${version}/api/`) to modify generated scaffolding patterns before first generation
- Commit generated files to git immediately after generation to track unintended overwrites
- For RFC 7807 errors, GeoIP, NanoID: implement as utilities in logic layer, not handlers

**Warning signs:**
- "DO NOT EDIT" header in file you're modifying
- Handler files containing business logic beyond basic request/response marshaling
- Main.go containing custom initialization code beyond what goctl generated
- ServiceContext modifications scattered across multiple files

**Phase to address:**
Phase 1 (Foundation Setup) — establish code generation discipline immediately. Phase 2 (Service Migration) — verify separation during Chi→go-zero migration.

---

### Pitfall 2: .api File Syntax Mistakes Blocking Code Generation

**What goes wrong:**
Subtle .api syntax errors prevent goctl from generating code, with cryptic error messages that don't point to the actual problem. Common issues: multiple service declarations, service blocks in imported files, unsupported generic types, inconsistent service naming across blocks.

**Why it happens:**
The .api DSL looks like Go but isn't. Developers assume Go struct syntax works everywhere, use generics, or organize imports incorrectly. Documentation shows examples but doesn't exhaustively list what's forbidden.

**How to avoid:**
- Run `goctl api validate --api <file>` after every .api edit
- Run `goctl api format --dir .` to auto-format and catch malformed syntax
- ONE service declaration per main .api file (can repeat same name to add routes)
- NO service blocks in imported .api files — only types/structs
- NO generics or weak types in .api syntax
- Keep all HTTP routes in main .api file, abstract structs in separate files
- Use `import` for shared types across services (e.g., ProblemDetail response)

**Warning signs:**
- Error message mentions "service" but you only declared one
- "Unsupported type" error on what looks like valid Go
- Code generation works for one .api file but not another with similar structure
- Import statements causing generation to fail

**Phase to address:**
Phase 1 (Foundation Setup) — validate .api syntax for both services before generating any code.

---

### Pitfall 3: zRPC Protobuf Schema Evolution Breaking Clients

**What goes wrong:**
Service A updates its .proto file (removes field, changes field number, removes field from oneof), regenerates code, deploys. Service B's zRPC clients suddenly fail with deserialization errors or silently read wrong data.

**Why it happens:**
Protobuf's wire format is forward/backward compatible ONLY if you follow strict rules. Developers treat .proto files like Go structs, freely renaming/removing fields without understanding field number permanence. Goctl's protoc wrapper (`goctl rpc protoc`) historically had issues with `--go_opt=M` options for import path mapping, making shared proto types harder.

**How to avoid:**
- NEVER change or reuse field numbers — reserve deleted field numbers forever
- NEVER remove fields — mark as `deprecated` instead and reserve the number
- ADD fields freely (safe for backward compatibility)
- Type changes: int32→int64 safe if values fit; other changes risky
- Use `reserved` keyword for deleted field numbers and names
- Deploy server schema changes BEFORE client schema changes
- Test compatibility with protolock or buf lint before deploying
- For shared types (URL, ClickEvent), put in dedicated repo and version via go modules

**Warning signs:**
- .proto file with gaps in field numbering (1, 2, 4, 5 — where's 3?)
- Field numbers changing between commits
- "File not found: gogo.proto" error (deprecated dependency)
- goctl complaining about `-M` option (use latest goctl version)
- Clients and servers deployed simultaneously during proto changes

**Phase to address:**
Phase 2 (Service Migration) — establish proto schema versioning before first zRPC call. Phase 3+ — enforce via CI checks (buf lint/protolock).

---

### Pitfall 4: SQLite→PostgreSQL Migration Losing Data Semantics

**What goes wrong:**
Migration script converts schema, data moves successfully, but application breaks because SQLite's loose typing allowed corrupted data (string in INTEGER column, date as text) that PostgreSQL rejects. Auto-increment behavior changes: SQLite reuses ROWID after DELETE, PostgreSQL SERIAL never reuses.

**Why it happens:**
SQLite's type affinity is forgiving — stores string in INTEGER column silently. PostgreSQL's strict typing surfaces silent corruption. Developers assume `INTEGER PRIMARY KEY` auto-increment behavior is universal, but SQLite's ROWID reuse differs from PostgreSQL SERIAL. Query syntax differs: SQLite uses `?` placeholders, PostgreSQL uses `$1, $2`.

**How to avoid:**
- Audit SQLite data BEFORE migration: `SELECT typeof(id), id FROM urls WHERE typeof(id) != 'integer'`
- Clean corrupted data first or migration will fail mid-flight
- Add CHECK constraints in PostgreSQL schema to prevent future corruption
- SQLite `INTEGER PRIMARY KEY AUTOINCREMENT` → PostgreSQL `BIGSERIAL` or `GENERATED ALWAYS AS IDENTITY`
- Replace `?` placeholders with `$1, $2, ...` in all SQL queries (sqlc handles this if schema migrated first)
- Enable foreign keys in SQLite for testing: `PRAGMA foreign_keys = ON` — catches referential integrity bugs before PostgreSQL migration
- Test migration on snapshot of production data, not just empty schema
- Use pgloader for schema conversion (handles boolean 0/1 → true/false, date text → timestamp)
- For go-zero: use sqlx with `db.Rebind()` or generate queries with sqlc for PostgreSQL

**Warning signs:**
- SQLite queries with `WHERE enabled = 1` (boolean as int)
- Date fields stored as TEXT or INTEGER in SQLite
- Foreign keys not enforced in SQLite (check with `PRAGMA foreign_keys`)
- Application logic parsing dates from strings instead of using time.Time
- Test suite passing with SQLite but not PostgreSQL

**Phase to address:**
Phase 2 (Database Migration) — audit and clean data, convert schema, update queries. Phase 3 — validate with production-like data.

---

### Pitfall 5: Kafka Consumer Rebalancing Storms During Deployment

**What goes wrong:**
Deploy new version of analytics-service with multiple replicas. All instances restart simultaneously, triggering cascading Kafka rebalances. Each JoinGroup request causes another rebalance, amplifying consumer lag. Message processing stalls for 30+ seconds.

**Why it happens:**
When all consumers in a group disconnect at once (rolling deployment), Kafka coordinator keeps rebalancing as consumers rejoin. Go-zero's `kq.MustNewQueue` with default `Consumers` and `Processors` may not align with partition count, causing uneven load. If message processing exceeds session timeout, consumer is kicked out mid-processing, triggering another rebalance.

**How to avoid:**
- Use incremental cooperative rebalancing (Kafka 2.4+) instead of eager rebalancing
- Configure session timeout > max message processing time (default: 10s, increase to 30s+ for analytics)
- Deploy with staggered restarts (not all replicas simultaneously)
- Set `kq` config: `Consumers` = number of goroutines pulling from Kafka (default: 8), `Processors` = number of goroutines processing messages (default: 8)
- Match `Processors` to number of partitions for the topic (e.g., 4 partitions → 4 processors)
- Use sticky assignor to minimize partition movement during rebalances
- Monitor rebalance events with Kafka metrics — alert if >5 rebalances/hour
- For go-zero: configure `kq.KqConf` with `CommitInterval: 1s` (default), increase for high-throughput low-criticality events
- Add graceful shutdown: drain in-flight messages before terminating consumer

**Warning signs:**
- Consumer lag spikes during deployments
- Logs showing multiple "JoinGroup" or "Rebalance" events within seconds
- Message processing appears frozen during deploys
- Error: "consumer kicked out of group" or "session timeout exceeded"
- Uneven partition assignment (one consumer handling 3 partitions, another handling 1)

**Phase to address:**
Phase 3 (Kafka Integration) — configure rebalancing strategy and consumer settings. Phase 4 (Production Hardening) — test deployment scenarios.

---

### Pitfall 6: Go-Zero ServiceContext Becoming Bloated Service Locator

**What goes wrong:**
ServiceContext starts clean: DB, Redis, Config. Over time, developers add every dependency: GeoIP reader, UA parser, NanoID generator, HTTP clients, caches, loggers. ServiceContext becomes a 500-line god object passed everywhere. Logic constructors have 8+ dependencies from svcCtx, making testing painful.

**Why it happens:**
ServiceContext is convenient — one place to put everything. Goctl-generated logic constructors receive `svcCtx *svc.ServiceContext`, encouraging developers to stuff everything in there. No framework guidance on what belongs vs. what should be scoped differently.

**How to avoid:**
- ServiceContext should ONLY hold infrastructure dependencies: DB, Redis, config, external API clients
- Request-scoped dependencies: pass via context.Context (e.g., tracing spans, request IDs)
- Stateless utilities: create as functions, not services (NanoID generator, UA parser)
- Per-logic dependencies: create in logic constructor, don't pass via ServiceContext (e.g., `NewShortCodeGenerator()` in logic, not svcCtx)
- Use constructor functions for logic that needs multiple dependencies from svcCtx, extract to smaller interfaces
- For testing: mock svcCtx fields individually, not entire ServiceContext
- If ServiceContext >200 lines, split into domain-specific contexts (URLServiceContext, AnalyticsServiceContext)

**Warning signs:**
- ServiceContext struct >15 fields
- ServiceContext containing function pointers (e.g., `GenerateID func() string`)
- Logic tests requiring mock of entire ServiceContext
- Logic constructors extracting 5+ fields from svcCtx
- ServiceContext containing "utility" functions (ParseUA, LookupGeoIP)

**Phase to address:**
Phase 2 (Service Migration) — establish ServiceContext boundaries early. Phase 4 — refactor if ServiceContext grew beyond infrastructure.

---

### Pitfall 7: Kafka Topic Misconfiguration Causing Message Loss

**What goes wrong:**
Analytics service publishes click events to Kafka topic with `retention.ms=3600000` (1 hour retention). Consumer goes down for 2 hours (deployment issue, crash). Events are deleted before consumer recovers. Silent data loss.

**Why it happens:**
Kafka defaults favor throughput over durability. Developers don't configure topic retention, replication, and acknowledgment appropriately for event criticality. Go-zero's `kq.Pusher` has default `chunkSize` and `flushInterval` that may batch messages too aggressively, delaying commits.

**How to avoid:**
- Set topic retention based on criticality: click events → 7 days minimum (allow reprocessing)
- Set `min.insync.replicas=2` to prevent data loss if broker fails
- For go-zero producer: configure `kq.NewPusher` with `WithChunkSize(100)` and `WithFlushInterval(1000ms)` — tune based on throughput
- Use producer acknowledgment: `acks=all` (wait for all replicas) for critical events
- Monitor consumer lag — alert if lag exceeds topic retention window
- Set topic partition count to match expected consumer parallelism (e.g., 4 partitions for 4 analytics instances)
- For URL shortener: click events are analytics, not transactional — 7-day retention acceptable. Deletion events may need longer retention (30 days).
- Add dead letter queue (DLQ) for messages that fail processing repeatedly

**Warning signs:**
- Consumer lag exceeding topic retention period
- Missing analytics events with no processing errors
- Kafka broker logs showing segment deletion while consumers are lagging
- Producer `send()` succeeding but messages not appearing in consumer
- Topic with `retention.ms=-1` (infinite) filling disk

**Phase to address:**
Phase 3 (Kafka Integration) — configure topic retention and replication before publishing first event.

---

### Pitfall 8: Dapr→Kafka Migration Changing Event Delivery Semantics

**What goes wrong:**
Dapr pub/sub (in-memory for dev) has at-most-once delivery. Kafka has at-least-once delivery. After migration, analytics service starts receiving duplicate click events. Analytics counts are inflated (same click counted multiple times).

**Why it happens:**
Dapr in-memory pub/sub drops messages on subscriber failure (fire-and-forget). Kafka retries failed messages, redelivers on consumer restart, and can deliver duplicates during rebalancing. Go-zero's `kq` consumer returns 200 even on processing errors (prevents retry storms), which means errors are silently dropped unless handled.

**How to avoid:**
- Design for idempotency: use event deduplication before processing (cache seen event IDs in Redis with TTL = retention period)
- For click events: use `{short_code}:{timestamp}:{ip}` as deduplication key (same user clicking twice within 1 second = duplicate)
- Kafka offset commit strategy: commit AFTER successful processing, not before (go-zero default: auto-commit every 1s)
- For analytics: accept duplicates, deduplicate at query time (SELECT DISTINCT) or use Redis INCR with SET NX for unique counts
- Add event versioning: `event_version: "v1"` in messages to support schema evolution during migration
- Test failure scenarios: kill consumer mid-processing, verify events are redelivered
- Go-zero kq consumer: handle errors explicitly, return errors to trigger retries (but be aware: returning 200 even on error is default behavior)

**Warning signs:**
- Analytics counts increasing after Kafka migration without traffic increase
- Duplicate events in Kafka topic (check with kafka-console-consumer)
- Consumer processing same event multiple times (check logs for duplicate event IDs)
- Click count != unique IP count for same short code

**Phase to address:**
Phase 3 (Kafka Integration) — implement idempotency before cutover from Dapr.

---

### Pitfall 9: Chi Middleware Incompatibility with Go-Zero

**What goes wrong:**
Developer ports Chi middleware (rate limiting, logging, CORS) directly to go-zero. Middleware doesn't run, or runs but breaks request handling. Middleware signature incompatibilities cause compile errors or runtime panics.

**Why it happens:**
Chi middleware uses `func(http.Handler) http.Handler`, while go-zero uses different middleware signatures in `rest.RunConf.ServiceConf.Middlewares`. Chi's `r.Context()` patterns don't translate directly. Chi's router context for URL params differs from go-zero's.

**How to avoid:**
- DO NOT copy-paste Chi middleware — rewrite for go-zero patterns
- Use go-zero middleware signature: defined in ServiceConf or as route-specific middleware
- For rate limiting: use go-zero's built-in `rest.WithMaxConns()` or Redis-based limiter
- For CORS: use go-zero's built-in `rest.WithCors()` configuration
- For logging: go-zero has built-in `logx` — integrate with existing logging, don't add custom middleware
- Extract middleware logic into functions, wrap in go-zero middleware adapter
- Test middleware in isolation before integrating into service

**Warning signs:**
- Middleware compiles but doesn't execute
- `http.Handler` type errors when adding middleware
- Request context values from Chi not accessible in go-zero handlers
- URL path parameters disappearing after middleware runs
- Panics in middleware due to nil context values

**Phase to address:**
Phase 2 (Service Migration) — rewrite middleware for go-zero before migrating routes.

---

### Pitfall 10: PostgreSQL Connection Pool Exhaustion

**What goes wrong:**
Service runs fine in dev with 10 concurrent requests. Deploy to production, traffic spikes, service returns "too many connections" errors. Database connection pool exhausted. Requests timeout waiting for available connections.

**Why it happens:**
Default PostgreSQL connection limits and default Go database pool settings don't match production traffic. SQLite had no connection pooling (single connection), so developers never tuned it. Go-zero's sqlx integration uses defaults: unlimited MaxOpenConns, which can exhaust database connection limits.

**How to avoid:**
- Set `MaxOpenConns` = (CPU cores * 2) + 1 as starting point
- Set `MaxIdleConns` = MaxOpenConns / 2 (keep pool warm)
- Set `ConnMaxLifetime` = 5 minutes (refresh connections periodically)
- Set `ConnMaxIdleTime` = 1 minute (close idle connections)
- Monitor connection usage: log pool stats (`db.Stats()`) in metrics
- PostgreSQL `max_connections` default is 100 — increase or reduce app pool size
- For go-zero: configure in `internal/config/config.go` DataSource settings
- Load test connection pool under expected traffic before deploying

**Warning signs:**
- "pq: sorry, too many clients already" errors
- Requests timing out under load but not in low traffic
- Database showing 100+ connections from single service instance
- Connection pool stats showing high wait count
- Service performance degrading as traffic increases

**Phase to address:**
Phase 2 (Database Migration) — configure pool settings during PostgreSQL setup. Phase 4 (Load Testing) — validate under production-like traffic.

---

## Technical Debt Patterns

| Shortcut | Immediate Benefit | Long-term Cost | When Acceptable |
|----------|-------------------|----------------|-----------------|
| Skipping .api validation | Faster iteration | Cryptic generation errors later | Never — validation takes <1s |
| Putting logic in handlers | Quick prototype | Code loss on regeneration | Never — logic files exist for this |
| Reusing protobuf field numbers | "Clean" numbering | Client/server deserialization failures | Never — field numbers are permanent |
| Single Kafka partition | Simple setup | No parallelism, consumer bottleneck | MVP only, must repartition before scale |
| In-memory rate limiting | No Redis dependency | Per-instance limits, resets on restart | Acceptable for MVP, migrate to Redis for production |
| Auto-commit Kafka offsets | Simpler code | Duplicate/lost messages on failure | Never for critical events, acceptable for analytics |
| SQLite type affinity | Flexible schema | Silent data corruption, PostgreSQL migration pain | Local dev only, never production |
| Ignoring consumer lag metrics | Less monitoring setup | Silent data loss when lag exceeds retention | Never — lag monitoring is critical |
| Copying Chi middleware as-is | Faster migration | Broken functionality, runtime errors | Never — rewrite for go-zero patterns |
| Using sqlc + go-zero model generation | Both tools for type safety | File conflicts, maintenance overhead | Choose one: sqlc for custom queries, go-zero for standard CRUD |

## Integration Gotchas

| Integration | Common Mistake | Correct Approach |
|-------------|----------------|------------------|
| go-zero + NanoID | Generate ID in handler | Generate in logic layer, pass via domain model |
| go-zero + RFC 7807 | Return error in handler | Create ProblemDetail in logic, serialize in handler |
| goctl + sqlc | Generate both, files conflict | Use sqlc for queries, go-zero for HTTP/RPC scaffolding |
| go-zero + GeoIP | Load MMDB in each logic instance | Load once in ServiceContext, reuse across requests |
| Kafka + go-zero | Use default Consumers=Processors=8 | Set Processors = partition count for even distribution |
| PostgreSQL + sqlx | Forget to call db.Rebind() | Use sqlc or call Rebind() for query placeholders |
| zRPC + shared protos | Copy .proto files between services | Use shared module, import via go.mod replace |
| Chi middleware → go-zero | Port middleware as-is | Rewrite as go-zero middleware (signature differs) |
| go-zero rate limiting | Implement custom per-instance limiter | Use built-in `rest.WithMaxConns()` or Redis-based global limiter |
| PostgreSQL timestamps | Use string parsing like SQLite | Use PostgreSQL `timestamptz`, Go `time.Time` |

## Performance Traps

| Trap | Symptoms | Prevention | When It Breaks |
|------|----------|------------|----------------|
| Single SQLite writer | Slow writes under load | Migrate to PostgreSQL with connection pooling | >100 concurrent writes |
| Unbounded ServiceContext | High memory per-instance | Lazy-load heavy resources (GeoIP MMDB) | ServiceContext >100MB |
| Kafka producer no batching | High latency, low throughput | Configure chunkSize and flushInterval | >1000 events/sec |
| PostgreSQL connection pool too small | Connection exhaustion errors | Set MaxOpenConns = (CPU cores * 2) + 1 | >50 concurrent requests |
| GeoIP MMDB loaded per-request | High latency, memory churn | Load once in ServiceContext | Every request +50ms |
| Kafka consumer session timeout too short | Constant rebalancing | Set session timeout > max processing time | Processing >10s |
| No Kafka partitioning strategy | Hot partition, uneven load | Partition by short_code hash | >10k events/sec |
| Protobuf without field limits | Memory exhaustion from huge messages | Set message size limits in zRPC config | Message >10MB |
| .api validation in CI only | Slow feedback loop | Run `goctl api validate` in pre-commit hook | Every commit |
| zRPC without connection pooling | Connection exhaustion | Use go-zero's built-in connection pooling | >1000 RPC calls/sec |

## Security Mistakes

| Mistake | Risk | Prevention |
|---------|------|------------|
| Exposing PostgreSQL credentials in go-zero config.yaml | Credential leak in git | Use environment variables, never commit credentials |
| No authentication on zRPC endpoints | Internal services exposed | Use zRPC interceptors for auth, or mTLS |
| Kafka topics without ACLs | Cross-service data access | Configure Kafka ACLs per-service |
| Rate limiting per-instance instead of global | Bypass by hitting multiple instances | Use Redis-based rate limiting (go-zero supports via redis-conf) |
| .proto files with sensitive fields | Data leak in logs/traces | Mark sensitive fields, redact in logs |
| No input validation in .api request types | Injection attacks | Add validation tags in .api types, validate in logic |
| SQLite database file world-readable | Data exposure | Set file permissions 0600, migrate to PostgreSQL with auth |
| Predictable short URL generation | Enumeration attacks | Use NanoID with 8+ characters, sufficient entropy |

## UX Pitfalls

| Pitfall | User Impact | Better Approach |
|---------|-------------|-----------------|
| Generic error responses after migration | Users see "internal error" for validation failures | Preserve RFC 7807 problem details in go-zero responses |
| Changed redirect latency (Dapr→zRPC) | Slower redirects noticed by users | Benchmark redirect path, optimize zRPC call or cache in Redis |
| Kafka async causing delay in analytics | Users expect real-time analytics, see stale data | Add "processing" indicator or use Kafka streams for real-time aggregation |
| Rate limit response changed format | API clients break | Keep RFC 7807 format for rate limit errors |
| PostgreSQL connection timeouts | Intermittent 500 errors | Set reasonable connection timeouts, add retries in logic |
| zRPC error messages too technical | Users see protobuf errors | Map zRPC errors to domain errors in logic layer |
| .api spec changes breaking API contract | Client apps break without warning | Version API routes (/v1/), maintain backward compatibility |

## "Looks Done But Isn't" Checklist

- [ ] **Code generation**: Generated handlers but didn't fill in logic files — verify all logic/*.go files have actual implementations
- [ ] **.api validation**: Created .api files but didn't run `goctl api validate` — verify no syntax errors
- [ ] **Protobuf reserved fields**: Defined .proto schema but didn't reserve deleted field numbers — check for `reserved` statements
- [ ] **Kafka topic creation**: Started Kafka consumer but topic doesn't exist (auto-created with wrong config) — verify topic created with correct retention/replication
- [ ] **PostgreSQL migration testing**: Ran schema migration but didn't test with production-like data — verify data semantics preserved
- [ ] **zRPC service registration**: Implemented zRPC service but didn't register in main.go — verify service appears in etcd/consul
- [ ] **ServiceContext cleanup**: Migrated to go-zero but kept Dapr dependencies in ServiceContext — verify old deps removed
- [ ] **Idempotency**: Integrated Kafka but didn't implement deduplication — verify duplicate events handled correctly
- [ ] **Error handling**: Migrated to go-zero but lost RFC 7807 format — verify problem details returned correctly
- [ ] **Connection pooling**: Migrated to PostgreSQL but forgot to configure pool settings — verify MaxOpenConns/MaxIdleConns set
- [ ] **Rate limiting**: Ported Chi rate limiter but it's per-instance — verify Redis-based global rate limiting
- [ ] **Middleware**: Migrated Chi middleware but go-zero signature differs — verify middleware rewritten for go-zero
- [ ] **Template customization**: Using default goctl templates — verify templates customized for RFC 7807, logging patterns
- [ ] **Kafka consumer graceful shutdown**: Consumer stops but doesn't drain in-flight messages — verify shutdown hook implemented

## Recovery Strategies

| Pitfall | Recovery Cost | Recovery Steps |
|---------|---------------|----------------|
| Overwritten logic files | MEDIUM | Restore from git, re-implement changes, commit before regenerating |
| Invalid .api syntax | LOW | Run `goctl api format`, fix validation errors, regenerate |
| Protobuf field number reuse | HIGH | Deploy hotfix with correct field numbers, clients must update simultaneously |
| SQLite data corruption | MEDIUM | Restore from backup, clean data, re-run migration with validation |
| Kafka consumer lag exceeds retention | MEDIUM | Accept data loss, update metrics from source of truth (DB), prevent recurrence |
| ServiceContext bloat | MEDIUM | Refactor logic tests to use interfaces, extract utilities, shrink ServiceContext incrementally |
| Kafka topic wrong retention | LOW | Alter topic config with kafka-configs.sh, verify with kafka-topics.sh |
| Duplicate events in analytics | LOW | Add deduplication logic, backfill correct counts from raw events |
| Lost RFC 7807 error format | LOW | Wrap logic errors in ProblemDetail before returning from handler |
| PostgreSQL connection exhaustion | LOW | Increase MaxOpenConns, add connection pool metrics, restart service |
| zRPC service not registered | LOW | Fix registration in main.go, redeploy, verify with service discovery |
| Chi middleware incompatible | MEDIUM | Rewrite middleware for go-zero signature, test with integration tests |
| Proto schema breaking change | HIGH | Rollback deployment, fix schema with backward compatibility, redeploy with feature flag |

## Pitfall-to-Phase Mapping

| Pitfall | Prevention Phase | Verification |
|---------|------------------|--------------|
| Code generation overwriting custom logic | Phase 1: Foundation Setup | Files with "DO NOT EDIT" header never modified in git history |
| .api syntax errors | Phase 1: Foundation Setup | `goctl api validate` passes in CI |
| Protobuf schema evolution breaking clients | Phase 2: Service Migration | Protolock/buf lint in CI, no field number changes |
| SQLite→PostgreSQL data loss | Phase 2: Database Migration | Audit queries pass, foreign keys enforced |
| Kafka consumer rebalancing storms | Phase 3: Kafka Integration | Deployment monitoring shows <5 rebalances, lag recovers <30s |
| ServiceContext bloat | Phase 2: Service Migration | ServiceContext <15 fields, logic tests use mocks |
| Kafka topic misconfiguration | Phase 3: Kafka Integration | Topics created with 7-day retention, replication factor 2 |
| Dapr→Kafka delivery semantics | Phase 3: Kafka Integration | Deduplication logic tested, duplicate events handled |
| Chi middleware incompatibility | Phase 2: Service Migration | Middleware rewritten and tested before route migration |
| PostgreSQL connection pool undersized | Phase 2: Database Migration | Load test shows no connection errors under expected traffic |
| Lost RFC 7807 error format | Phase 2: Service Migration | Integration tests verify ProblemDetail response format |
| Rate limiting per-instance | Phase 4: Production Hardening | Rate limiting uses Redis, verified with multi-instance test |
| zRPC authentication missing | Phase 4: Production Hardening | zRPC interceptors configured, unauthorized calls rejected |

## Sources

**Go-zero framework:**
- [GitHub - zeromicro/go-zero](https://github.com/zeromicro/go-zero)
- [go-zero Documentation](https://go-zero.dev/en/)
- [API specification | go-zero Documentation](https://go-zero.dev/en/docs/tutorials)
- [API file format | go-zero Documentation](https://go-zero.dev/en/docs/tasks/dsl/api)
- [goctl api | go-zero Documentation](https://go-zero.dev/en/docs/tutorials/cli/api)
- [Template customization | go-zero Documentation](https://go-zero.dev/en/docs/tutorials/customization/template)
- [Message queue | go-zero Documentation](https://go-zero.dev/en/docs/tutorials/message-queue/kafka)
- [Developing a RESTful API With Go (go-zero)](https://medium.com/better-programming/developing-a-restful-api-with-go-b5150f693277)

**Code generation issues:**
- [github.com/gogo/protobuf/gogoproto/gogo.proto: File not found Issue #3652](https://github.com/zeromicro/go-zero/issues/3652)
- [goctl rpc protoc missing M option Issue #1585](https://github.com/zeromicro/go-zero/issues/1585)
- [How to reference common structs in .api files Issue #1059](https://github.com/zeromicro/go-zero/issues/1059)

**SQLite to PostgreSQL migration:**
- [How to migrate from SQLite to PostgreSQL](https://render.com/articles/how-to-migrate-from-sqlite-to-postgresql)
- [Database Migration: SQLite to PostgreSQL](https://www.bytebase.com/blog/database-migration-sqlite-to-postgresql/)
- [SQLite Autoincrement](https://www.sqlite.org/autoinc.html)

**PostgreSQL connection pooling:**
- [How to Implement Connection Pooling in Go for PostgreSQL](https://oneuptime.com/blog/post/2026-01-07-go-postgresql-connection-pooling/view)
- [Go Database Connection Pool: Efficient Management with sqlx](https://tech-champion.com/database/postgresql/efficient-go-database-connection-pool-management-with-sqlx/)

**Kafka integration:**
- [Why Golang Devs Get Kafka Error Handling Wrong](https://caffeinatedcoder.medium.com/why-golang-devs-get-kafka-error-handling-wrong-and-how-to-fix-it-5c9b0f3fb66a)
- [Kafka Rebalancing: Triggers, Effects, and Mitigation](https://www.redpanda.com/guides/kafka-performance-kafka-rebalancing)
- [Understanding Consumer Groups, Partition Assignments, and Rebalances in Kafka](https://www.danielsobrado.com/blog/understanding-consumer-groups-partition-assignments-and-rebalances/)
- [Cooperative Rebalancing in Kafka](https://www.confluent.io/blog/cooperative-rebalancing-in-kafka-streams-consumer-ksqldb/)

**Protobuf schema evolution:**
- [Protocol Buffers Schema Evolution Guide - Backward Compatibility 2025](https://jsontotable.org/blog/protobuf/protobuf-schema-evolution)
- [Protocol Buffers Best Practices for Backward and Forward Compatibility](https://earthly.dev/blog/backward-and-forward-compatibility/)
- [Understanding Protobuf Compatibility](https://yokota.blog/2021/08/26/understanding-protobuf-compatibility/)

**Rate limiting:**
- [Rate limiting in Go using token bucket](https://golang.org/x/time/rate)
- [How to Implement Rate Limiting in Go Without External Services](https://oneuptime.com/blog/post/2026-01-07-go-rate-limiting/view)

**Dependency injection:**
- [Dependency Injection in Go: Patterns & Best Practices](https://medium.com/@rosgluk/dependency-injection-in-go-patterns-best-practices-5e5136df5357)

---
*Pitfalls research for: go-zero microservices adoption*
*Researched: 2026-02-15*
